{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126efb20",
   "metadata": {},
   "source": [
    "# Document Classification with Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c2fe2",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f392dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer, precision_score, recall_score, precision_recall_fscore_support, f1_score \n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e564",
   "metadata": {},
   "source": [
    "### Importing and Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49742410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    390\n",
      "1    210\n",
      "Name: doc_topics, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Labelled Sampled Speeches.csv\")\n",
    "df = df.dropna(subset=['doc_topics'])\n",
    "\n",
    "X = df['speech_text']  # features\n",
    "y = df['doc_topics']   # classification label\n",
    "\n",
    "# number of documents in each topic\n",
    "category_counts = df['doc_topics'].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "# split the data: 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703fbeee",
   "metadata": {},
   "source": [
    "### Baseline Accuracy\n",
    "##### Random Assignment of Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aeac8b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.1\n"
     ]
    }
   ],
   "source": [
    "# counting the number of instances in each class\n",
    "random.seed(1)\n",
    "N_hostility = sum(1 for label in y_train if label == 1)\n",
    "N_hospitality = sum(1 for label in y_train if label == 2)\n",
    "\n",
    "# generating random predictions\n",
    "random_predictions = [random.choice([1, 2]) for doc_topics in range(len(y_train))]\n",
    "\n",
    "# calculating accuracy based on random predictions\n",
    "accuracy_random = round(np.mean(np.array(random_predictions) == np.array(y_train)),3)*100\n",
    "print(accuracy_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f3f18",
   "metadata": {},
   "source": [
    "##### Dictionary Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d78231fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set:\n",
      "Accuracy: 60.0\n",
      "F1 Score: 62.5\n",
      "Precision: 83.3\n",
      "Recall: 50.0\n",
      "\n",
      "Training Set:\n",
      "Accuracy: 60.4\n",
      "F1 Score: 61.5\n",
      "Precision: 82.6\n",
      "Recall: 49.0\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = [\"hon\", \"rose—\", \"rose\", \"government\", \"minister\",\n",
    "                   \"gentleman\", \"speaker\", \"mr\", \"friend\", \"home\", \"secretary\",\n",
    "                   \"friend\", \"right\", \"<\", \">\", \"can\", \"lady\", \"people\"]\n",
    "\n",
    "def preprocess_text_dict(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]  \n",
    "    tokens = [token for token in tokens if token not in custom_stopwords]  \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X_train_dictmethod = [preprocess_text_dict(text) for text in X_train]\n",
    "X_test_dictmethod = [preprocess_text_dict(text) for text in X_test]\n",
    "y_train_binary = [1 if label == 2 else 0 for label in y_train]\n",
    "y_test_binary = [1 if label == 2 else 0 for label in y_test]\n",
    "\n",
    "# \"Hostile\" dictionary\n",
    "host_words = [\"illegal\", \"offend\", \"border\", \"control\", \"genuine\",\n",
    "              \"model\", \"system\", \"deport\", \"point\", \"detention\", \n",
    "              \"migrant\", \"bogus\", \"issue\", \"france\",\"smugglers\"]\n",
    "\n",
    "# \"Hospitable\" dictionary\n",
    "hosp_words = [\"welcome\", \"safe\", \"trauma\", \"flee\", \"visa\", \n",
    "              \"help\", \"war\", \"support\", \"brutal\", \"hostile\",\n",
    "              \"women\", \"children\", \"vulnerable\",\"refugee\", \"integrate\"]\n",
    "\n",
    "def count_words(text, word_set):\n",
    "    count = 0\n",
    "    for word in text.split():\n",
    "        if word in word_set:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# count occurrences of hostile and hospitable words in each document\n",
    "X_train_hostile_count = [count_words(text, set(host_words)) for text in X_train_dictmethod]\n",
    "X_train_hospitable_count = [count_words(text, set(hosp_words)) for text in X_train_dictmethod]\n",
    "X_test_hostile_count = [count_words(text, set(host_words)) for text in X_test_dictmethod]\n",
    "X_test_hospitable_count = [count_words(text, set(hosp_words)) for text in X_test_dictmethod]\n",
    "\n",
    "# classification function\n",
    "def classify_documents(X_train_hostile_count, X_train_hospitable_count):\n",
    "    predictions = []\n",
    "    for hosp_count, hostile_count in zip(X_train_hospitable_count, X_train_hostile_count):\n",
    "        if hosp_count > hostile_count:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n",
    "\n",
    "# predictions for training and testing sets\n",
    "y_train_pred_dict = classify_documents(X_train_hostile_count, X_train_hospitable_count)\n",
    "y_test_pred_dict = classify_documents(X_test_hostile_count, X_test_hospitable_count)\n",
    "\n",
    "# evaluating the model on training set using dictionary method\n",
    "train_accuracy_dict, train_precision_dict, train_recall_dict, train_f1_dict = evaluate_model(y_train_binary, y_train_pred_dict)\n",
    "test_accuracy_dict, test_precision_dict, test_recall_dict, test_f1_dict = evaluate_model(y_test_binary, y_test_pred_dict)\n",
    "\n",
    "# evaluating dictionary method\n",
    "print(\"Testing Set:\")\n",
    "print(\"Accuracy:\", round(100*test_accuracy_dict,1))\n",
    "print(\"F1 Score:\", round(100*test_f1_dict,1))\n",
    "print(\"Precision:\", round(100*test_precision_dict,1))\n",
    "print(\"Recall:\", round(100*test_recall_dict,1))\n",
    "\n",
    "print(\"\\nTraining Set:\")\n",
    "print(\"Accuracy:\", round(100*train_accuracy_dict,1))\n",
    "print(\"F1 Score:\", round(100*train_f1_dict,1))\n",
    "print(\"Precision:\", round(100*train_precision_dict,1))\n",
    "print(\"Recall:\", round(100*train_recall_dict,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba32b5f",
   "metadata": {},
   "source": [
    "### Pre-processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc1bbf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "custom_stopwords = [\"hon\", \"rose—\", \"rose\", \"government\", \"minister\",\n",
    "                   \"gentleman\", \"speaker\", \"mr\", \"friend\", \"home\", \"secretary\",\n",
    "                   \"friend\", \"right\", \"<\", \">\", \"can\", \"lady\", \"people\"]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Remove non-alphabetic characters\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in custom_stopwords]  # Remove custom stopwords\n",
    "    tokens = [stemmer.stem(token) for token in tokens]  # Stemming\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# preprocessing text data for training and testing sets\n",
    "X_train_preprocessed = [preprocess_text(text) for text in X_train]\n",
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
    "\n",
    "# defining feature representations\n",
    "vectorizers = [\n",
    "    CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=600),\n",
    "    CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=900),\n",
    "    CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=1200),\n",
    "    CountVectorizer(min_df=5, lowercase=True, ngram_range=(1, 2), max_features=600),\n",
    "    CountVectorizer(min_df=5, lowercase=True, ngram_range=(1, 2), max_features=900),\n",
    "    CountVectorizer(min_df=5, lowercase=True, ngram_range=(1, 2), max_features=1200),\n",
    "    TfidfVectorizer(min_df=5, lowercase=True, max_features=600),\n",
    "    TfidfVectorizer(min_df=5, lowercase=True, max_features=900),\n",
    "    TfidfVectorizer(min_df=5, lowercase=True, max_features=1200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec2cac",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbf4a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy  F1 Score  Precision  Recall  \\\n",
      "0      Unigram                 600      82.5      79.9       80.6    79.4   \n",
      "1      Unigram                 900      83.3      81.2       81.2    81.2   \n",
      "2      Unigram                1200      80.0      77.2       77.6    76.9   \n",
      "3       Bigram                 600      81.7      79.6       79.3    80.0   \n",
      "4       Bigram                 900      82.5      80.2       80.4    80.0   \n",
      "5       Bigram                1200      82.5      80.4       80.3    80.6   \n",
      "6       TF-IDF                 600      75.0      65.9       76.1    65.0   \n",
      "7       TF-IDF                 900      77.5      70.9       77.8    69.4   \n",
      "8       TF-IDF                1200      78.3      72.8       77.8    71.2   \n",
      "\n",
      "       Parameters  \n",
      "0  {'alpha': 0.7}  \n",
      "1  {'alpha': 0.3}  \n",
      "2  {'alpha': 0.5}  \n",
      "3  {'alpha': 0.7}  \n",
      "4  {'alpha': 0.5}  \n",
      "5  {'alpha': 0.7}  \n",
      "6  {'alpha': 0.1}  \n",
      "7  {'alpha': 0.1}  \n",
      "8  {'alpha': 0.1}  \n"
     ]
    }
   ],
   "source": [
    "# creating a MNB cross validation function\n",
    "param_grid = {\n",
    "    'alpha': [0.05,0.08,0.1,0.3,0.5,0.7,1.0],  # Smoothing parameter (alpha)\n",
    "}\n",
    "\n",
    "def MultinomialNB_train_and_evaluate(X_train, X_test, y_train, y_test, param_grid):\n",
    "    grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=10, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_nb_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    y_pred = best_nb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    return accuracy, f1, precision, recall, best_params\n",
    "\n",
    "results = []      \n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy, f1, precision, recall, chosen_parameters = MultinomialNB_train_and_evaluate(X_train_dtm, X_test_dtm, y_train, y_test, param_grid)\n",
    " \n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy,3)*100,\n",
    "        \"F1 Score\": round(f1,3)*100,\n",
    "        \"Precision\": round(precision,3)*100,\n",
    "        \"Recall\": round(recall,3)*100,\n",
    "        \"Parameters\": chosen_parameters\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"nb_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523ff29",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes on Training Set (Testing for Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69fd1865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy  F1 Score  Precision  Recall  \\\n",
      "0      Unigram                 600      85.8      85.0       84.3    86.2   \n",
      "1      Unigram                 900      88.5      88.0       87.2    89.9   \n",
      "2      Unigram                1200      89.8      89.2       88.4    90.8   \n",
      "3       Bigram                 600      84.8      83.9       83.2    85.0   \n",
      "4       Bigram                 900      87.7      87.0       86.3    88.6   \n",
      "5       Bigram                1200      89.6      89.0       88.2    90.7   \n",
      "6       TF-IDF                 600      89.2      87.7       89.9    86.3   \n",
      "7       TF-IDF                 900      91.7      90.6       92.3    89.4   \n",
      "8       TF-IDF                1200      94.8      94.2       95.1    93.4   \n",
      "\n",
      "       Parameters  \n",
      "0  {'alpha': 0.7}  \n",
      "1  {'alpha': 0.3}  \n",
      "2  {'alpha': 0.5}  \n",
      "3  {'alpha': 0.7}  \n",
      "4  {'alpha': 0.5}  \n",
      "5  {'alpha': 0.7}  \n",
      "6  {'alpha': 0.1}  \n",
      "7  {'alpha': 0.1}  \n",
      "8  {'alpha': 0.1}  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy, f1, precision, recall, chosen_parameters = MultinomialNB_train_and_evaluate(X_train_dtm, X_train_dtm, y_train, y_train, param_grid)\n",
    "\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy, 3)*100,\n",
    "        \"F1 Score\": round(f1, 3)*100,\n",
    "        \"Precision\": round(precision, 3)*100,\n",
    "        \"Recall\": round(recall, 3)*100,\n",
    "        \"Parameters\": chosen_parameters\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"nb_training_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce925fd5",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4345fa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy  F1 Score  Precision  Recall  \\\n",
      "0      Unigram                 600      80.0      73.9       82.5    71.9   \n",
      "1      Unigram                 900      80.8      74.7       84.6    72.5   \n",
      "2      Unigram                1200      80.8      74.7       84.6    72.5   \n",
      "3       Bigram                 600      79.2      72.5       81.7    70.6   \n",
      "4       Bigram                 900      78.3      71.1       81.0    69.4   \n",
      "5       Bigram                1200      80.0      73.3       84.0    71.2   \n",
      "6       TF-IDF                 600      76.7      65.6       87.0    65.0   \n",
      "7       TF-IDF                 900      76.7      65.6       87.0    65.0   \n",
      "8       TF-IDF                1200      76.7      66.5       83.7    65.6   \n",
      "\n",
      "      Parameters  \n",
      "0  {'C': [0.05]}  \n",
      "1  {'C': [0.04]}  \n",
      "2  {'C': [0.04]}  \n",
      "3  {'C': [0.05]}  \n",
      "4  {'C': [0.04]}  \n",
      "5  {'C': [0.04]}  \n",
      "6   {'C': [1.0]}  \n",
      "7   {'C': [1.0]}  \n",
      "8   {'C': [1.0]}  \n"
     ]
    }
   ],
   "source": [
    "def logit_train_and_evaluate(X_train, X_test, y_train, y_test, custom_Cs=None):\n",
    "    if custom_Cs is None:\n",
    "        custom_Cs = [0.01, 0.02, 0.03, 0.04, 0.05, 1.0]\n",
    "\n",
    "    lasso_logit_model = LogisticRegressionCV(Cs=custom_Cs, scoring=\"f1\")  \n",
    "    lasso_logit_model.fit(X_train, y_train) \n",
    "    y_pred = lasso_logit_model.predict(X_test)  \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro',zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    chosen_parameters = {\n",
    "        \"C\": lasso_logit_model.C_,\n",
    "    }\n",
    "\n",
    "    return accuracy, f1, precision, recall, conf_matrix, chosen_parameters\n",
    "\n",
    "results = []\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\" \n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy, f1, precision, recall, conf_matrix, chosen_parameters = logit_train_and_evaluate(X_train_dtm, X_test_dtm, y_train, y_test)\n",
    "\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy,3)*100,\n",
    "        \"F1 Score\": round(f1,3)*100,\n",
    "        \"Precision\": round(precision,3)*100,\n",
    "        \"Recall\": round(recall,3)*100,\n",
    "        \"Parameters\": chosen_parameters\n",
    "    })\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"lr_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23098c6",
   "metadata": {},
   "source": [
    "### Logistic Regression on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc590b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy (Train)  F1 Score (Train)  \\\n",
      "0      Unigram                 600              92.3              91.1   \n",
      "1      Unigram                 900              91.7              90.3   \n",
      "2      Unigram                1200              91.9              90.6   \n",
      "3       Bigram                 600              92.3              91.1   \n",
      "4       Bigram                 900              91.9              90.6   \n",
      "5       Bigram                1200              92.3              91.1   \n",
      "6       TF-IDF                 600              89.4              87.5   \n",
      "7       TF-IDF                 900              89.4              87.4   \n",
      "8       TF-IDF                1200              89.0              86.9   \n",
      "\n",
      "   Precision (Train)  Recall (Train) Parameters (Train)  \n",
      "0               94.7            89.1      {'C': [0.05]}  \n",
      "1               94.3            88.2      {'C': [0.04]}  \n",
      "2               94.4            88.5      {'C': [0.04]}  \n",
      "3               94.2            89.4      {'C': [0.05]}  \n",
      "4               94.4            88.5      {'C': [0.04]}  \n",
      "5               94.7            89.1      {'C': [0.04]}  \n",
      "6               92.3            85.3       {'C': [1.0]}  \n",
      "7               92.9            85.0       {'C': [1.0]}  \n",
      "8               92.4            84.5       {'C': [1.0]}  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\" \n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy_train, f1_train, precision_train, recall_train, conf_matrix_train, chosen_parameters_train = logit_train_and_evaluate(X_train_dtm, X_train_dtm, y_train, y_train)\n",
    "\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy (Train)\": round(accuracy_train, 3) * 100,\n",
    "        \"F1 Score (Train)\": round(f1_train, 3) * 100,\n",
    "        \"Precision (Train)\": round(precision_train, 3) * 100,\n",
    "        \"Recall (Train)\": round(recall_train, 3) * 100,\n",
    "        \"Parameters (Train)\": chosen_parameters_train,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "results_df.to_excel(\"lr_training_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581d85d",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfae5732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy  F1 Score  Precision  Recall  \\\n",
      "0      Unigram                 600      79.2      71.9       83.4    70.0   \n",
      "1      Unigram                 900      80.0      73.3       84.0    71.2   \n",
      "2      Unigram                1200      79.2      71.9       83.4    70.0   \n",
      "3       Bigram                 600      78.3      70.4       82.7    68.8   \n",
      "4       Bigram                 900      79.2      71.9       83.4    70.0   \n",
      "5       Bigram                1200      80.8      74.7       84.6    72.5   \n",
      "6       TF-IDF                 600      76.7      70.7       75.4    69.4   \n",
      "7       TF-IDF                 900      79.2      73.6       79.5    71.9   \n",
      "8       TF-IDF                1200      78.3      71.7       79.7    70.0   \n",
      "\n",
      "                        Parameters  \n",
      "0  {'kernel': 'linear', 'nu': 0.6}  \n",
      "1  {'kernel': 'linear', 'nu': 0.6}  \n",
      "2  {'kernel': 'linear', 'nu': 0.6}  \n",
      "3  {'kernel': 'linear', 'nu': 0.6}  \n",
      "4  {'kernel': 'linear', 'nu': 0.6}  \n",
      "5  {'kernel': 'linear', 'nu': 0.6}  \n",
      "6  {'kernel': 'linear', 'nu': 0.6}  \n",
      "7  {'kernel': 'linear', 'nu': 0.6}  \n",
      "8  {'kernel': 'linear', 'nu': 0.6}  \n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nu': [0.6,0.65,0.7],  # nu parameters\n",
    "    'kernel': ['linear'],  # kernel function = linear\n",
    "}\n",
    "\n",
    "def SVM_train_and_evaluate(X_train, X_test, y_train, y_test, param_grid):\n",
    "    grid_search = GridSearchCV(NuSVC(), param_grid, cv=10, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_nusvm_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    y_pred = best_nusvm_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    return accuracy, f1, precision, recall, best_params\n",
    "\n",
    "results = []\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy, f1, precision, recall, chosen_parameters = SVM_train_and_evaluate(X_train_dtm, X_test_dtm, y_train, y_test, param_grid)\n",
    "\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy,3)*100,\n",
    "        \"F1 Score\": round(f1,3)*100,\n",
    "        \"Precision\": round(precision,3)*100,\n",
    "        \"Recall\": round(recall,3)*100,\n",
    "        \"Parameters\": chosen_parameters\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"svm_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e4905",
   "metadata": {},
   "source": [
    "### Support Vector Machine on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5edf8f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy (Train)  F1 Score (Train)  \\\n",
      "0      Unigram                 600              89.4              87.4   \n",
      "1      Unigram                 900              90.2              88.5   \n",
      "2      Unigram                1200              90.6              89.0   \n",
      "3       Bigram                 600              88.8              86.6   \n",
      "4       Bigram                 900              89.6              87.6   \n",
      "5       Bigram                1200              90.4              88.7   \n",
      "6       TF-IDF                 600              92.3              91.3   \n",
      "7       TF-IDF                 900              94.2              93.4   \n",
      "8       TF-IDF                1200              96.0              95.6   \n",
      "\n",
      "   Precision (Train)  Recall (Train)               Parameters (Train)  \n",
      "0               92.6            85.1  {'kernel': 'linear', 'nu': 0.6}  \n",
      "1               93.1            86.3  {'kernel': 'linear', 'nu': 0.6}  \n",
      "2               93.7            86.8  {'kernel': 'linear', 'nu': 0.6}  \n",
      "3               92.3            84.3  {'kernel': 'linear', 'nu': 0.6}  \n",
      "4               93.1            85.3  {'kernel': 'linear', 'nu': 0.6}  \n",
      "5               93.5            86.5  {'kernel': 'linear', 'nu': 0.6}  \n",
      "6               93.3            89.9  {'kernel': 'linear', 'nu': 0.6}  \n",
      "7               95.4            92.0  {'kernel': 'linear', 'nu': 0.6}  \n",
      "8               96.9            94.5  {'kernel': 'linear', 'nu': 0.6}  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy_train, f1_train, precision_train, recall_train, chosen_parameters_train = SVM_train_and_evaluate(X_train_dtm, X_train_dtm, y_train, y_train, param_grid)\n",
    "\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy (Train)\": round(accuracy_train, 3) * 100,\n",
    "        \"F1 Score (Train)\": round(f1_train, 3) * 100,\n",
    "        \"Precision (Train)\": round(precision_train, 3) * 100,\n",
    "        \"Recall (Train)\": round(recall_train, 3) * 100,\n",
    "        \"Parameters (Train)\": chosen_parameters_train\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"svm_training_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1b595",
   "metadata": {},
   "source": [
    "## Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b602df",
   "metadata": {},
   "source": [
    "### Making Word Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d4bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "X_train_preprocessed = [preprocess_text(text) for text in X_train]\n",
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
    "\n",
    "# Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "def encode_document(document, word2vec_model):\n",
    "    words = document.split()\n",
    "    word_vectors = [word2vec_model[word] for word in words if word in word2vec_model]\n",
    "\n",
    "    return word_vectors\n",
    "\n",
    "# aggregate word vectors for document representation\n",
    "def average_word_vectors(word_vectors):\n",
    "    if not word_vectors:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "X_train_encoded = [encode_document(doc, word2vec_model) for doc in X_train_preprocessed]\n",
    "X_test_encoded = [encode_document(doc, word2vec_model) for doc in X_test_preprocessed]\n",
    "X_train_avg = [average_word_vectors(doc) for doc in X_train_encoded]\n",
    "X_test_avg = [average_word_vectors(doc) for doc in X_test_encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35986e5",
   "metadata": {},
   "source": [
    "### Logistic Regression with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f674695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.5\n",
      "F1 Score: 58.5\n",
      "Precision: 76.0\n",
      "Recall: 47.5\n",
      "Best Hyperparameter (C): [30]\n",
      " \n",
      "Testing for Overfitting on the Training Set\n",
      "Accuracy: 86.2\n",
      "F1 Score: 79.0\n",
      "Precision: 86.1\n",
      "Recall: 72.9\n",
      "Best Hyperparameter (C): [30]\n"
     ]
    }
   ],
   "source": [
    "custom_Cs = [10, 15, 20, 30, 40]\n",
    "\n",
    "logit_model = LogisticRegressionCV(\n",
    "    Cs=custom_Cs,\n",
    "    cv=10,          \n",
    "    random_state=123, \n",
    "    max_iter=1000, # to ensure the model converges\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "\n",
    "\n",
    "logit_model.fit(X_train_avg, y_train) # training\n",
    "y_pred = logit_model.predict(X_test_avg) # predictions\n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "best_C = logit_model.C_ # best hyperparameters\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score* 100:.1f}\")\n",
    "print(f\"Precision: {precision* 100:.1f}\")\n",
    "print(f\"Recall: {recall* 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (C): {best_C}\")\n",
    "\n",
    "# testing for overfitting\n",
    "y_pred_train = logit_model.predict(X_train_avg)\n",
    "accuracy = accuracy_score(y_train, y_pred_train)  # accuracy\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_train, y_pred_train, average='binary')\n",
    "\n",
    "print(\" \")\n",
    "print(\"Testing for Overfitting on the Training Set\")\n",
    "print(f\"Accuracy: {accuracy * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score * 100:.1f}\")\n",
    "print(f\"Precision: {precision * 100:.1f}\")\n",
    "print(f\"Recall: {recall * 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (C): {best_C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f45162",
   "metadata": {},
   "source": [
    "### SVM with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537bed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.3\n",
      "F1 Score: 61.8\n",
      "Precision: 75.0\n",
      "Recall: 52.5\n",
      "Best Hyperparameter (nu): 0.4\n",
      "Best Hyperparameter (kernel): poly\n",
      " \n",
      "Results on the Training Set:\n",
      "Accuracy: 95.0\n",
      "F1 Score: 92.7\n",
      "Precision: 95.6\n",
      "Recall: 90.0\n",
      "Best Hyperparameter (nu): 0.4\n",
      "Best Hyperparameter (kernel): poly\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nu': [0.4,0.5,0.6,0.7],  # nu parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # kernel functions\n",
    "}\n",
    "\n",
    "nu_svm_model = NuSVC()\n",
    "grid_search = GridSearchCV(nu_svm_model, param_grid, cv=10,scoring=\"f1\") #cv\n",
    "grid_search.fit(X_train_avg, y_train)\n",
    "best_nu = grid_search.best_params_['nu'] \n",
    "best_kernel = grid_search.best_params_['kernel']\n",
    "\n",
    "y_pred_nu_svm = grid_search.predict(X_test_avg) # predictions\n",
    "accuracy_nu_svm = accuracy_score(y_test, y_pred_nu_svm) \n",
    "precision_nu_svm, recall_nu_svm, f1_score_nu_svm, _ = precision_recall_fscore_support(y_test, y_pred_nu_svm, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_nu_svm* 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score_nu_svm* 100:.1f}\")\n",
    "print(f\"Precision: {precision_nu_svm* 100:.1f}\")\n",
    "print(f\"Recall: {recall_nu_svm* 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (nu): {best_nu}\")\n",
    "print(f\"Best Hyperparameter (kernel): {best_kernel}\")\n",
    "\n",
    "# testing for overfitting\n",
    "y_pred_nu_svm_train = grid_search.predict(X_train_avg)\n",
    "accuracy_nu_svm_train = accuracy_score(y_train, y_pred_nu_svm_train)\n",
    "precision_nu_svm_train, recall_nu_svm_train, f1_score_nu_svm_train, _ = precision_recall_fscore_support(\n",
    "    y_train, y_pred_nu_svm_train, average='binary'\n",
    ")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Results on the Training Set:\")\n",
    "print(f\"Accuracy: {accuracy_nu_svm_train * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score_nu_svm_train * 100:.1f}\")\n",
    "print(f\"Precision: {precision_nu_svm_train * 100:.1f}\")\n",
    "print(f\"Recall: {recall_nu_svm_train * 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (nu): {best_nu}\")\n",
    "print(f\"Best Hyperparameter (kernel): {best_kernel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16170501",
   "metadata": {},
   "source": [
    "### Logistic Regression with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd08518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.7\n",
      "F1 Score: 61.1\n",
      "Precision: 68.8\n",
      "Recall: 55.0\n",
      "Best Hyperparameter (C): [0.5]\n",
      "\n",
      "Testing for Overfitting on the Training Set\n",
      "Accuracy: 89.4\n",
      "F1 Score: 84.1\n",
      "Precision: 89.4\n",
      "Recall: 79.4\n",
      "Best Hyperparameter (C): [0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "custom_Cs = [0.4,0.5,0.7,1.0]\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# BERT tokeniser and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "X_train_bert = []\n",
    "X_test_bert = []\n",
    "\n",
    "# get BERT embeddings for training data\n",
    "for text in X_train:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    avg_embeddings = torch.mean(last_hidden_states, dim=1).squeeze().numpy()  # Average pooling of token embeddings\n",
    "    X_train_bert.append(avg_embeddings)\n",
    "\n",
    "# get BERT embeddings for testing data\n",
    "for text in X_test:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    avg_embeddings = torch.mean(last_hidden_states, dim=1).squeeze().numpy()  # Average pooling of token embeddings\n",
    "    X_test_bert.append(avg_embeddings)\n",
    "\n",
    "X_train_bert = np.array(X_train_bert)\n",
    "X_test_bert = np.array(X_test_bert)\n",
    "\n",
    "# Logistic Regression + BERT\n",
    "logit_model = LogisticRegressionCV(\n",
    "    Cs=custom_Cs,\n",
    "    cv=10,           \n",
    "    random_state=123, \n",
    "    max_iter=1000,    \n",
    "    scoring='f1_macro'\n",
    ")\n",
    "\n",
    "\n",
    "logit_model.fit(X_train_bert, y_train) # Training\n",
    "y_pred = logit_model.predict(X_test_bert) # Predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "best_C = logit_model.C_\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score* 100:.1f}\")\n",
    "print(f\"Precision: {precision* 100:.1f}\")\n",
    "print(f\"Recall: {recall* 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (C): {best_C}\")\n",
    "\n",
    "# Testing for Overfitting\n",
    "y_pred_train = logit_model.predict(X_train_bert)\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_train, y_pred_train, average='binary')\n",
    "print(\"\\nTesting for Overfitting on the Training Set\")\n",
    "print(f\"Accuracy: {accuracy * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score * 100:.1f}\")\n",
    "print(f\"Precision: {precision * 100:.1f}\")\n",
    "print(f\"Recall: {recall * 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (C): {best_C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "597876b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m last_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     35\u001b[0m avg_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(last_hidden_states, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Average pooling of token embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1007\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1008\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1009\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1027\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    595\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    596\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:531\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    528\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    529\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 531\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:543\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 543\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:443\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 443\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "custom_Cs = [0.05,0.1,0.5,1]\n",
    "np.random.seed(123)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "X_train_bert = []\n",
    "X_test_bert = []\n",
    "\n",
    "# get BERT embeddings for training data\n",
    "for text in X_train:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    avg_embeddings = torch.mean(last_hidden_states, dim=1).squeeze().numpy()  # Average pooling of token embeddings\n",
    "    X_train_bert.append(avg_embeddings)\n",
    "\n",
    "# get BERT embeddings for testing data\n",
    "for text in X_test:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    avg_embeddings = torch.mean(last_hidden_states, dim=1).squeeze().numpy()  # Average pooling of token embeddings\n",
    "    X_test_bert.append(avg_embeddings)\n",
    "\n",
    "X_train_bert = np.array(X_train_bert)\n",
    "X_test_bert = np.array(X_test_bert)\n",
    "\n",
    "# SVM + BERT\n",
    "svm_model = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'C': custom_Cs,\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # kernel functions\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=10, scoring='f1_macro')\n",
    "grid_search.fit(X_train_bert, y_train)\n",
    "best_C = grid_search.best_params_['C']\n",
    "best_kernel = grid_search.best_params_['kernel']\n",
    "y_pred_svm = grid_search.predict(X_test_bert)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm, recall_svm, f1_score_svm, _ = precision_recall_fscore_support(y_test, y_pred_svm, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_svm * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score_svm* 100:.1f}\")\n",
    "print(f\"Precision: {precision_svm* 100:.1f}\")\n",
    "print(f\"Recall: {recall_svm* 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (C): {best_C}\")\n",
    "print(f\"Best Kernel: {best_kernel}\")\n",
    "\n",
    "# Testing for Overfitting\n",
    "y_pred_svm_train = grid_search.predict(X_train_bert)\n",
    "accuracy_svm_train = accuracy_score(y_train, y_pred_svm_train)\n",
    "precision_svm_train, recall_svm_train, f1_score_svm_train, _ = precision_recall_fscore_support(y_train, y_pred_svm_train, average='binary')\n",
    "print(\"\\nTesting for Overfitting on the Training Set\")\n",
    "print(f\"Accuracy: {accuracy_svm_train * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score_svm_train * 100:.1f}\")\n",
    "print(f\"Precision: {precision_svm_train * 100:.1f}\")\n",
    "print(f\"Recall: {recall_svm_train * 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (C): {best_C}\")\n",
    "print(f\"Best Kernel: {best_kernel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde25356",
   "metadata": {},
   "source": [
    "## Chosen Model: Naive Bayes, Unigram, 900 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ed7b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.read_csv(\"debate_dataset.csv\")\n",
    "fullX = fulldf['speech_text'] \n",
    "fullX_preprocessed = [preprocess_text(text) for text in fullX]\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=900)\n",
    "X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_full = vectorizer.transform(fullX_preprocessed)\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_dtm, y_train)\n",
    "y_pred_full = naive_bayes_classifier.predict(X_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96756426",
   "metadata": {},
   "source": [
    "## Labelling the rest of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22b4c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf['predicted_label'] = y_pred_full\n",
    "fulldf.to_csv(\"fulldf_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e109721",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d5182",
   "metadata": {},
   "source": [
    "#### Printing Correctly Classified Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137836a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=600)\n",
    "X_train_dtm = vectorizers.fit_transform(X_train_preprocessed)\n",
    "X_test_dtm = vectorizers.transform(X_test_preprocessed)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB(alpha=0.7)\n",
    "naive_bayes_classifier.fit(X_train_dtm, y_train)\n",
    "x_pred_chosenmodel = naive_bayes_classifier.predict(X_test_dtm)\n",
    "correct_indices = [i for i, (pred, actual) in enumerate(zip(x_pred_chosenmodel, y_test)) if pred == actual]\n",
    "\n",
    "print(\"Correctly classified documents:\")\n",
    "for idx in correct_indices[:50]: \n",
    "    print(f\"Document {idx + len(X_train)}:\")\n",
    "    print(X_test_preprocessed[idx])  \n",
    "    print(\"Predicted Label:\", x_pred_chosenmodel[idx]) \n",
    "    print(\"Actual Label:\", y_test.iloc[idx])  \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2505627",
   "metadata": {},
   "source": [
    "#### Getting the most important terms for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=600)\n",
    "X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "# Train Multinomial Naive Bayes classifier\n",
    "naive_bayes_classifier = MultinomialNB(alpha=0.7)\n",
    "naive_bayes_classifier.fit(X_train_dtm, y_train)\n",
    "feature_names = vectorizer.get_feature_names_out() # get feature names\n",
    "log_probabilities = naive_bayes_classifier.feature_log_prob_ # get the log probabilities\n",
    "\n",
    "top_n = 30\n",
    "for i, target_class in enumerate(naive_bayes_classifier.classes_):\n",
    "    print(f\"Top {top_n} features for class {target_class}:\")\n",
    "    class_probabilities = log_probabilities[i]  # log probabilities for current class\n",
    "    top_indices = class_probabilities.argsort()[-top_n:][::-1] \n",
    "    top_features = [feature_names[index] for index in top_indices]  # top features\n",
    "    print(top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a32938",
   "metadata": {},
   "source": [
    "#### Printing Incorrectly Classified Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_indices = [i for i, (pred, actual) in enumerate(zip(x_pred_chosenmodel, y_test)) if pred != actual]\n",
    "print(\"Incorrectly classified documents:\")\n",
    "for idx in incorrect_indices[:10]:  \n",
    "    print(f\"Document {idx + len(X_train)}:\")\n",
    "    print(X_test_preprocessed[idx])  \n",
    "    print(\"Predicted Label:\", x_pred_chosenmodel[idx]) \n",
    "    print(\"Actual Label:\", y_test.iloc[idx]) \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
